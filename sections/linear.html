<h2><code>Linear(X)</code></h2>
<p><strong>Description:</strong> Applies a linear transformation to the input.</p>

<h3>Parameters</h3>
<ul>
<li><code>X</code> (<em>Variable</em>): The input tensor, must be of shape \(N \times D\). It typically represents a batch of \(N\) instances of size \(D\).</li>
</ul>

<h3>Returns</h3>
<p><code>Y</code> (<em>Variable</em>): A tensor of size \(N \times C\).</p>

<h3>Theory</h3>

<p>
    This Module contains two parameters: a tensor of weights \(W \in \mathbb{R}^{D \times C}\) as well as a bias \(b \in \mathbb{R}^{C}\). Given the input \(X\), it outputs \(Y = X \times W + b\), where addition of the bias term is broadcasted in the batch dimension. In terms of the backwards pass, we've done all the hard work already! The output can be computed by using add() and matmul(), which already track gradients. 
</p>

<h3>Code</h3>
<pre class="line-numbers item" style="font-size: 0.70em;"><code class="language-python">class Linear(Module):
    # Module initialization.
    def __init__(self, input_size, output_size, param_init="xavier"):
        super().__init__()

        # Store hyper_parameters for print_module()
        self._hyper_parameters["input_size"] = input_size
        self._hyper_parameters["output_size"] = output_size
        self._hyper_parameters["param_init"] = param_init

        # Bias initialized to 0.
        self.b = Parameter(np.zeros(output_size), keep_grad=True)

        # Weights are initialized using xavier initialization.
        if param_init == "xavier":
            xavier_range = np.sqrt(6 / (1 + input_size + output_size))
            self.W = Parameter(
                np.random.uniform(
                    -xavier_range, xavier_range, size=(input_size, output_size)
                ),
                keep_grad=True,
            )

    def forward(self, X):

        X = matmul(X, self.W)
        X = add(X, self.b)

        return X</code></pre>
